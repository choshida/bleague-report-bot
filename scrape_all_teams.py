import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import logging

logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')

# ãƒãƒ¼ãƒ è¨­å®šï¼ˆç‰çƒï¼RSSã€ä»–2ãƒãƒ¼ãƒ ï¼HTMLï¼‰
TEAMS = {
    "ç‰çƒã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚­ãƒ³ã‚°ã‚¹": {
        "type": "rss",
        "rss_url": "https://goldenkings.jp/news/rss.xml"
    },
    "èŒ¨åŸãƒ­ãƒœãƒƒãƒ„": {
        "type": "scrape",
        "url": "https://www.ibarakirobots.win/news/",
        "base_url": "https://www.ibarakirobots.win",
        "item_selector": ".news-item",
        "title_selector": ".title",
        "date_selector": ".date",
        "link_inside": "a"
    },
    "ä¿¡å·ãƒ–ãƒ¬ã‚¤ãƒ–ã‚¦ã‚©ãƒªã‚¢ãƒ¼ã‚º": {
        "type": "scrape",
        "url": "https://www.b-warriors.net/news/",
        "base_url": "https://www.b-warriors.net",
        "item_selector": "ul.news__list > li",
        "title_selector": ".news__list-title",
        "date_selector": ".news__list-date",
        "link_inside": "a"
    }
}

# RSSå–å¾—ï¼ˆUser-Agentä»˜ãï¼‰
def scrape_from_rss(team_name, rss_url):
    logging.info(f"ğŸ“¡ {team_name}ï¼ˆRSSï¼‰")
    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; BLeagueBot/1.0)"
    }

    try:
        response = requests.get(rss_url, headers=headers, timeout=10)
        if response.status_code != 200:
            logging.warning(f"{team_name} RSSå–å¾—å¤±æ•—ï¼š{response.status_code}")
            return

        root = ET.fromstring(response.content)
        items = root.findall(".//item")
        logging.info(f"{team_name} RSSè¨˜äº‹æ•°: {len(items)}")

        for item in items[:5]:
            title = item.find("title").text
            pubDate = item.find("pubDate").text
            link = item.find("link").text
            print(f"{pubDate}ï½œ{title}\nâ†’ {link}\n")

    except Exception as e:
        logging.error(f"{team_name} RSSå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ï¼š{e}")

# HTMLå–å¾—
def scrape_from_html(team_name, config):
    logging.info(f"ğŸ€ {team_name}ï¼ˆHTMLï¼‰")
    try:
        res = requests.get(config["url"], timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        items = soup.select(config["item_selector"])
        logging.info(f"{team_name} HTMLè¨˜äº‹æ•°: {len(items)}")

        for item in items[:5]:
            title_tag = item.select_one(config["title_selector"])
            date_tag = item.select_one(config["date_selector"])
            link_tag = item.select_one(config["link_inside"])

            if title_tag and date_tag and link_tag:
                title = title_tag.get_text(strip=True)
                date = date_tag.get_text(strip=True)
                url = config["base_url"] + link_tag["href"]
                print(f"{date}ï½œ{title}\nâ†’ {url}\n")

    except Exception as e:
        logging.error(f"{team_name} HTMLå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ï¼š{e}")

# å®Ÿè¡Œæœ¬ä½“
if __name__ == "__main__":
    for team_name, config in TEAMS.items():
        logging.info(f"[DEBUG] å‡¦ç†ä¸­: {team_name}, type: {config.get('type')}")

        if config.get("type") == "rss":
            scrape_from_rss(team_name, config.get("rss_url", ""))
        elif config.get("type") == "scrape":
            scrape_from_html(team_name, config)
        else:
            logging.warning(f"{team_name} ã®typeãŒä¸æ˜ã¾ãŸã¯æœªè¨­å®š")
