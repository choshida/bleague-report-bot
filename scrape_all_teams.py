import requests
import feedparser
from bs4 import BeautifulSoup

# ãƒãƒ¼ãƒ è¨­å®šï¼štypeã«å¿œã˜ã¦rss or scrapeã‚’ä½¿ã„åˆ†ã‘
TEAMS = {
    "ç‰çƒã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚­ãƒ³ã‚°ã‚¹": {
        "type": "rss",
        "rss_url": "https://goldenkings.jp/news/rss.xml"
    },
    "èŒ¨åŸãƒ­ãƒœãƒƒãƒ„": {
        "type": "scrape",
        "url": "https://www.ibarakirobots.win/news/",
        "base_url": "https://www.ibarakirobots.win",
        "item_selector": ".news-item",
        "title_selector": ".title",
        "date_selector": ".date",
        "link_inside": "a"
    },
    "ä¿¡å·ãƒ–ãƒ¬ã‚¤ãƒ–ã‚¦ã‚©ãƒªã‚¢ãƒ¼ã‚º": {
        "type": "scrape",
        "url": "https://www.b-warriors.net/news/",
        "base_url": "https://www.b-warriors.net",
        "item_selector": "ul.news__list > li",
        "title_selector": ".news__list-title",
        "date_selector": ".news__list-date",
        "link_inside": "a"
    }
}

# RSSæ–¹å¼
def scrape_from_rss(team_name, rss_url):
    print(f"\nğŸ“¡ {team_name}ï¼ˆRSSï¼‰")
    print("-" * 40)
    feed = feedparser.parse(rss_url)
    if not feed.entries:
        print("âš ï¸ RSSãŒç©ºã§ã™")
    for entry in feed.entries[:5]:
        published = entry.get("published", "æ—¥ä»˜ä¸æ˜")
        title = entry.get("title", "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜")
        link = entry.get("link", "URLä¸æ˜")
        print(f"{published}ï½œ{title}")
        print(f"â†’ {link}\n")

# HTMLã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ–¹å¼
def scrape_from_html(team_name, config):
    print(f"\nğŸ€ {team_name}ï¼ˆHTMLï¼‰")
    print("-" * 40)
    try:
        res = requests.get(config["url"], timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        items = soup.select(config["item_selector"])
        print(f"ğŸ” è¦ç´ æ•°: {len(items)}")

        if not items:
            print("âš ï¸ ãƒ‹ãƒ¥ãƒ¼ã‚¹é …ç›®ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return

        for item in items[:5]:
            title_tag = item.select_one(config["title_selector"])
            date_tag = item.select_one(config["date_selector"])
            link_tag = item.select_one(config["link_inside"])

            if title_tag and date_tag and link_tag:
                title = title_tag.get_text(strip=True)
                date = date_tag.get_text(strip=True)
                url = config["base_url"] + link_tag["href"]

                print(f"{date}ï½œ{title}")
                print(f"â†’ {url}\n")

    except Exception as e:
        print(f"[ERROR] {team_name} ã®å–å¾—ã«å¤±æ•—ï¼š{e}")

# å®Ÿè¡Œã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
if __name__ == "__main__":
    for team_name, config in TEAMS.items():
        if config["type"] == "rss":
            scrape_from_rss(team_name, config["rss_url"])
        elif config["type"] == "scrape":
            scrape_from_html(team_name, config)
        else:
            print(f"âš ï¸ {team_name} ã®typeè¨­å®šãŒä¸æ˜ã§ã™")
